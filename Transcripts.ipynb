{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "import requests\n",
    "from requests_html import HTMLSession, AsyncHTMLSession\n",
    "from bs4 import BeautifulSoup\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. List of S&P100 companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wikiurl = \"https://en.wikipedia.org/wiki/S%26P_100\"\n",
    "table_class = \"wikitable sortable jquery-tablesorter\"\n",
    "response = requests.get(wikiurl)\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "sandp100 = soup.find_all('table')\n",
    "df = pd.read_html(str(sandp100))\n",
    "df = pd.DataFrame(df[2])\n",
    "df.to_csv('sandp100.csv', index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Get links to all transcripts for each S&P 100 company\n",
    "## Use seekingalpha's transcript page to get all the transcript pages for each company. Save to csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session = HTMLSession()\n",
    "list_of_post_urls = []\n",
    "for i, row in df.iterrows():\n",
    "    print(i, row['Symbol'])\n",
    "    url = 'https://seekingalpha.com/symbol/{}/earnings/transcripts'.format(row['Symbol'])\n",
    "    r = session.get(url)\n",
    "    fn = 'html/post_urls/{}.html'.format(row['Symbol'])\n",
    "    with open(fn, 'wb') as f:\n",
    "        f.write(r.content)\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = 'html/post_urls/'\n",
    "post_html_files = os.listdir(path)\n",
    "d = {}\n",
    "for fn in post_html_files:\n",
    "    if fn.endswith('.html'):\n",
    "        symbol, _ = fn.split('.html')\n",
    "        with open(path+fn, 'r') as f:\n",
    "            soup = BeautifulSoup(f, 'html.parser')\n",
    "        posts = soup.find_all('article')\n",
    "        base_url = 'https://seekingalpha.com'\n",
    "        for p in posts:\n",
    "            title = p.find('a').text\n",
    "            if any(x in title for x in ['Earnings Call Transcript', 'Earning Call Transcript']):\n",
    "                abs_url = base_url + p.find('a').attrs['href'].split('?')[0]\n",
    "                dt = p.find('span').text\n",
    "                d[title] = [abs_url, dt, symbol]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "post_urls = pd.DataFrame.from_dict(d, orient='index')\n",
    "post_urls = post_urls.reset_index()\n",
    "post_urls = post_urls.rename(columns={'index':'title', 0: 'post_url', 1:'date', 2:'symbol'})\n",
    "post_urls['date'] = post_urls['date'].str[5:]\n",
    "post_urls['date'] = post_urls['date'].str.replace('.','')\n",
    "post_urls['date'] = post_urls['date'].str.replace(',','')\n",
    "post_urls['date'] = pd.to_datetime(post_urls['date'], format='%b %d %Y')\n",
    "post_urls.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "post_urls.to_csv('post_urls.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Filter for transcripts since 2021 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "post_urls = pd.read_csv('post_urls.csv', parse_dates=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "posts_in_scope = post_urls[post_urls['date'].dt.year >= 2021]\n",
    "posts_in_scope.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(posts_in_scope)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Asynchronously query each transcript page for every company (~1100 pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "async def get_html(title, post_url, symbol, date):\n",
    "    r = await asession.get(post_url)\n",
    "    r.title = title\n",
    "    r.symbol = symbol\n",
    "    r.date = date\n",
    "    return r\n",
    "\n",
    "for sym in posts_in_scope['symbol'].unique():   \n",
    "    print(sym)\n",
    "    reqs = []\n",
    "    masked = posts_in_scope[posts_in_scope['symbol'] == sym]\n",
    "    urls = masked.to_dict(orient='records')\n",
    "    asession = AsyncHTMLSession()\n",
    "    result = asession.run(*[lambda d=d: get_html(d['title'],\n",
    "                                                 d['post_url'],\n",
    "                                                 d['symbol'],\n",
    "                                                 d['date']) for d in urls])\n",
    "    for r in result:\n",
    "        l = [r.symbol, r.date.strftime('%Y-%m-%d'), r.title, '.html']\n",
    "        fn = '_'.join(l)\n",
    "        fp = 'html/posts/'\n",
    "        with open(fp+fn, 'wb') as f:\n",
    "            f.write(r.content)\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this is the non-async method\n",
    "# session = HTMLSession()\n",
    "# for i, row in posts_in_scope.iterrows():\n",
    "#     print(row['title'])\n",
    "#     fn = row['title']+'.html'\n",
    "#     fp = 'html/posts/'\n",
    "#     if fn not in os.listdir(fp):\n",
    "#         r = session.get(row['post_url'])\n",
    "#         with open(fp+fn, 'wb') as f:\n",
    "#             f.write(r.content)\n",
    "#         time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Parse out transcript text and save to text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = 'html/posts/'\n",
    "post_files = os.listdir(path)\n",
    "d = {}\n",
    "for fn in post_files:\n",
    "    if fn.endswith('.html'):\n",
    "        with open(path+fn, 'r') as f:\n",
    "            soup = BeautifulSoup(f, 'html.parser')\n",
    "        divs = soup.find_all('div', class_='hp_h6')\n",
    "        text = divs[0].text\n",
    "        with open('transcripts/{}.txt'.format(fn[:-6]), 'w') as f:\n",
    "            f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
